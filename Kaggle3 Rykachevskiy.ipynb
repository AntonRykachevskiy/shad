{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Идеи и ход решения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для итоговой модели я использовал три представления данных: исходное представление в виде sparse матрицы, те же данные после PCA на 500 компонент (число компонент выбрано случайно) и матрица нормированных по строке корреляций тестового датасета с обобщенными портретами классов построенным по обучающей выборке. (считается корреляция строки тестового датасета с средним значением строки в обучающей выборке для всех примеров, где присутствует тот или иной тег).\n",
    "\n",
    "### Модели:\n",
    "\n",
    "- SGD с функцией потерь 'modified huber' на исходном представлении\n",
    "- MultinomialBayes из коробки на исходном представлении\n",
    "- RF на 500 деревьев на матрице корреляций с портретами классов\n",
    "- LinearRegression из коробки для предсказания количества тегов для каждого документа обучаемая на PCA представлении данных\n",
    "\n",
    "Итоговый ответ собирался из взвешенных предсказаний SGD + MultinomianBayes + RF + нормированные корреляции\n",
    "{'MNB': 9.7144514654701197e-17,\n",
    " 'RF': 0.64999999999999991,\n",
    " 'SGD': 0.050000000000000003,\n",
    " 'Correlations': 0.29999999999999999}, тут видно, что MNB не играет, но на кросс валидации был вариант весов в котором MNB имел реальный вес, а модель проигрывала в тысячных, так что возможно MNB имеет смысл. \n",
    " \n",
    "###  Кроссвалидация и настройка параметров\n",
    "Кроссвалидация на трех фолдах написана руками. \n",
    "RF учится на данных полученных полученных как результат другой модели(корреляций), так что внутри каждого из 3-х фолдов кроссвалидации происходит разбиение еще на 5 фолдов и по 4-м считаются значения корреляций для 5-ой, потом из этого собирается таблица корреляций для всего внешнего фолда и на нем учится RF. Портреты считаются тоже по всему внешнему фолду. \n",
    "\n",
    "Решение устроенно так, чтобы один раз посчитать таблицы корреляций, предсказания RF, SGD и MNB для трех фолдов, а потом подобрать весовые коэффициенты между моделями. \n",
    "\n",
    "--я пробовал добавить еще одно рабиение на фолды внутри каждого из этих трех, чтобы обучить линейную метамодель на всех предсказаниях, но результат улучшить не вышло. \n",
    "\n",
    "Помимо этого обучается линейная регрессия чтобы предсказывать количество тегов, и результат предсказания округляется  так, чтобы получить лучший результат на кросс валидации. В таком варианте получается, что общее количество предсказанных тегов приблизительно совпадает с реальным. В качестве ответа выбирается такое количество элементов с максимальными вероятностями, которое предсказывает регрессия\n",
    "--в целом этот шаг дает не очень большой прирост, кажется, несколько тысячных. Тут я пробовал много подходов. Изначально просто выбирал 1 тег с максимальной вероятностью, так же пробовал устанавливать порог по вероятности, пробовал везде выбирать один тег по максимальной вероятности, а второй добавлять, только если он выше порога, но изменения от всех таких подходов не велики.\n",
    "\n",
    "\n",
    "### Что не сработало\\не успел\n",
    "\n",
    "- Предсказывать как отдельные классы парные, тройные,... шестерные теги, тогда всего получается около 600 классов. Я пробовал тут только RF + корреляции с не очень честной кросс валидацией просто по отдельному сету, там подход себя показал неудачно и я его оставил. С другой стороны 6 тегов не встречаются сами по себе, а бывают только в паре с другим, так что возможно сюда надо было копать дальше.\n",
    "- Предсказывать первый тег, потом перекидывать его в признак и предсказывать второй. Вроде не сработало, но это было в 6 утра перед дедлайном, так что я не уверен, а сейчас нет времени проверить.\n",
    "- PCA на 98 компонент - ничего хорошего\n",
    "- LogReg и SVM на исходном сете - SGD с huber loss лучше\n",
    "- Попытаться корреляции как-то калибровать в вероятности - не успел\n",
    "- Вместо просто суммирования для обобщенных портретов строить суммы квадратов, это увеличивает вес значимых слов относительно остальных - кажется это дает небольшое улучение, но я не уверен. \n",
    "- добавлял куда-то какие-то сборные фичи, не помогало, забил\n",
    "\n",
    "### P.S. \n",
    "Я не запускал все в этом ноутбуке это долго, просто подчистил и структурировал тот в котором решал, так что сходу что-то может не взлететь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Готовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def read_data_from_file(filename, shape):\n",
    "    values = list()\n",
    "    rows = list()\n",
    "    cols = list()\n",
    "    \n",
    "    header = True\n",
    "    for line in open(filename):\n",
    "        if header:\n",
    "            header = False\n",
    "            continue\n",
    "        row, col, value = [x for x in line.strip().split(',')]\n",
    "        row, col = int(row), int(col)\n",
    "        value = float(value)\n",
    "        row -= 1\n",
    "        col -= 1\n",
    "        values.append(value)\n",
    "        rows.append(row)\n",
    "        cols.append(col)\n",
    "        \n",
    "    return sparse.csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 30000) (15000, 30000)\n"
     ]
    }
   ],
   "source": [
    "X_train = read_data_from_file('X_train.csv', (15000, 30000)).astype(float)\n",
    "X_test = read_data_from_file('X_test.csv', (15000, 30000)).astype(float)\n",
    "print X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labels_from_file(filename, shape):\n",
    "    labels = np.zeros(shape).astype(int)\n",
    "\n",
    "    header = True\n",
    "    for line in open(filename):\n",
    "        if header:\n",
    "            header = False\n",
    "            continue\n",
    "        row, indeces = line.strip().split(',')\n",
    "        row = int(row) - 1\n",
    "        indeces = [int(x) - 1 for x in indeces.split()]\n",
    "        labels[row, indeces] = 1\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 98)\n"
     ]
    }
   ],
   "source": [
    "y_train = read_labels_from_file('y_train.csv', (15000, 98))\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_labels_to_file(labels, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    print >> outfile, \"Id,Labels\"\n",
    "    for i, line in enumerate(labels):\n",
    "        elements = [str(x) for x in list(np.nonzero(line)[0] + 1)]\n",
    "        \n",
    "        print >> outfile, \"%d,%s\" % (i + 1, ' '.join(elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделяем часть датасета для кросс валидации/обучения метаалгоритмов/подбора параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 30000)\n",
      "(10500, 98)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train_sampled, X_cv, y_train_sampled, Y_cv = train_test_split(X_train, y_train, test_size=0.30, random_state=0)\n",
    "print X_train_sampled.shape\n",
    "print y_train_sampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=500, n_iter=5,\n",
       "       random_state=239, tol=0.0)"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=500, random_state=239)\n",
    "svd.fit(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 500)"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_PCA = svd.transform(X_train)\n",
    "X_test = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_PCA = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 500)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_PCA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попробуем выделить пары категорий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реально выделять пары, тройки и прочее не понадобится, но от этой части решения останется представление данных в виде уникальных идентификаторов классов. Список из 599 списков, каждый из которых содержит одно из возможных сочетаний тегов, первые 92 элемента это варианты с одним тегом, потом около сотни с двумя тегами итд до 7 тегов. Больше 7 в учебном сете нет. Соответственно тут функции чтобы отлавливать уникальные наборы тегов и перекодировать Y из стандартного представление в такое и наоборот.\n",
    "\n",
    "Важный результат этого куска - список all_nplexes - набор уникальных сочетаний, он понадобится потом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def catchNplex(y, n_components = 2):\n",
    "    y_nplexes = y[y.sum(1) == n_components]\n",
    "    \n",
    "    nplexes = list()\n",
    "    \n",
    "    for nplex in y_nplexes:\n",
    "        #print type(nplex)\n",
    "        if list(nplex) in nplexes:\n",
    "            pass\n",
    "        else:\n",
    "            nplexes.append(list(nplex))\n",
    "    \n",
    "    return nplexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_nplexes = []\n",
    "    \n",
    "for n in range(0, 7):\n",
    "    all_nplexes += catchNplex(y_train, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_nplexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fromYtoNplex(y_to_transform, all_nplexes):\n",
    "    y_transformed = np.zeros((y_to_transform.shape[0], len(all_nplexes)))\n",
    "    \n",
    "    for i in range(y_to_transform.shape[0]):\n",
    "        nplex_num = np.where(np.all(all_nplexes == y_to_transform[i], axis=1))[0][0]\n",
    "        \n",
    "        y_transformed[i][nplex_num] = 1\n",
    "    \n",
    "    return y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fromNplextoY(y_to_transform, all_nplexes):\n",
    "    y = []\n",
    "    empty = [0 for i in range(98)]\n",
    "    for i in range(y_to_transform.shape[0]):\n",
    "        #print np.where(y_to_transform[i] == 1)\n",
    "        if sum(y_to_transform[i]) != 0:\n",
    "            y.append(all_nplexes[np.where(y_to_transform[i] == 1)[0][0]])\n",
    "        else:\n",
    "            y.append(empty)\n",
    "        \n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_trainsformed = fromYtoNplex(y_train, all_nplexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = fromNplextoY(y_train_trainsformed, all_nplexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class LRClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, n_jobs=-1, t = 0.1, nplexes = all_nplexes):\n",
    "        self.C = C\n",
    "        self.n_jobs = n_jobs\n",
    "        self.nplexes = nplexes\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #y_transformed = fromYtoNplex(y, self.nplexes)\n",
    "        self.clf = OneVsRestClassifier(LogisticRegression(C = self.C)\n",
    "                                       , self.n_jobs)\n",
    "        self.clf.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #y = fromNplextoY(self.clf.predict(X), self.nplexes)\n",
    "        return self.clf.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, kernel = 'linear', n_jobs=-1, nplexes = all_nplexes):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.n_jobs = n_jobs\n",
    "        self.nplexes = nplexes\n",
    "    def fit(self, X, y):\n",
    "        y_transformed = fromYtoNplex(y, self.nplexes)\n",
    "        self.clf = OneVsRestClassifier(SVC(C=self.C, kernel = self.kernel, probability=True, cache_size = 10000), self.n_jobs)\n",
    "        self.clf.fit(X, y_transformed)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class RFClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_estimators=100, n_jobs=-1, nplexes = all_nplexes):\n",
    "        self.n_estimators=n_estimators\n",
    "        self.n_jobs = n_jobs\n",
    "        self.nplexes = nplexes\n",
    "    def fit(self, X, y):\n",
    "        #y_transformed = fromYtoNplex(y, self.nplexes)\n",
    "        self.clf = OneVsRestClassifier(RandomForestClassifier(self.n_estimators, n_jobs = self.n_jobs)\n",
    "                                       , self.n_jobs)\n",
    "        self.clf.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "class SGDMulticlassClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, n_jobs=-1):\n",
    "        self.C = C\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.clf = OneVsRestClassifier(SGDClassifier(loss='modified_huber')\n",
    "                                       , self.n_jobs)\n",
    "        self.clf.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Пара вспомогательных функций для подготовки ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "sound_file = './call.wav'\n",
    "\n",
    "ALLERT = Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"./call.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALLERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocessWithT(y_test, t):\n",
    "    y_fin = np.zeros(y_test.shape) \n",
    "    for i in range(y_test.shape[0]):\n",
    "        for j in range(y_test.shape[1]):\n",
    "            if y_test[i][j] >= t:\n",
    "                y_fin[i][j] = 1\n",
    "    return y_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LinReg = LinearRegression()\n",
    "LinReg.fit(X_train_sampled_PCA, y_len_train_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def produceOutByLen(prop, lens):\n",
    "    ans = np.zeros(prop.shape)\n",
    "    print ans.shape\n",
    "    for i in range(len(lens)):\n",
    "        for j in range(lens[i]): \n",
    "            ans[i][np.argsort(-prop[i])[j]] = 1\n",
    "    return ans     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создадим портреты классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот кусочек про портреты. \n",
    "\n",
    "createPortraits(  ) создает портреты по списку вроде all_nplexes и обучающей выборке\n",
    "\n",
    "predictSimple(  ) считает корреляции для тестового сета\n",
    "\n",
    "portaitsCycle(  ) создает портреты, потом считает корреляции, потом нормирует и еще и дает оценку результата, если предсказывать просто по корреляциям тег с максимальной корреляцией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createPortraits(nplexes_set, X, y, plot = False):\n",
    "    portraits = []\n",
    "\n",
    "    for i in range(nplexes_set.shape[0]):\n",
    "        mask = np.all(nplexes_set[i] == y, axis =1)\n",
    "\n",
    "        if any(mask):\n",
    "            try:\n",
    "                raw_masked = (np.array(X[mask].sum(0))[0])/sum(np.array(X[mask].sum(0))[0])\n",
    "            except:\n",
    "                print X.shape\n",
    "                print mask\n",
    "            portraits.append(list(raw_masked))\n",
    "        else:\n",
    "            portraits.append(list(np.zeros(X.shape[1])))\n",
    "           \n",
    "        if plot:\n",
    "            plt.plot(raw_masked)\n",
    "\n",
    "            #plt.plot(np.histogram(np.array(raw_masked)[0], bins=50)[1][1:],\n",
    "            #    np.histogram(np.array(raw_masked)[0], bins=50)[0])\n",
    "\n",
    "            #plt.plot(X_train_sampled[mask].sum(0))\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "    return np.array(portraits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictSimple(X_to_pred, portrs, rmse = False):\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(X_to_pred.shape[0]):\n",
    "        cu = np.nan_to_num(X_to_pred[i].toarray()/X_to_pred[i].sum())\n",
    "\n",
    "        corrs = []\n",
    "        for p in portrs:\n",
    "            if rmse:\n",
    "                corrs.append(mean_squared_error(np.nan_to_num(p.reshape(1, -1)), cu))\n",
    "            else:\n",
    "                corrs.append(np.corrcoef(p, cu)[0][1])\n",
    "\n",
    "        y_pred.append(corrs)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to(y_single, nplexes):\n",
    "    ans = np.zeros((len(y_single), len(nplexes)))\n",
    "    for i in range(len(y_single)):\n",
    "        ans[i][y_single[i]] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def portraitsCycle(train, train_label, test, test_label, nplex_set, plot = False):\n",
    "    portraits = createPortraits(nplex_set, train, train_label, plot)\n",
    "    simple_pred = np.array(predictSimple(test, portraits))\n",
    "    simple_pred = np.nan_to_num(simple_pred)\n",
    "    simple_ans = fromNplextoY(to(simple_pred.argmax(1), nplex_set), nplex_set)\n",
    "    print f1_score(test_label, simple_ans, average='samples')\n",
    "    \n",
    "    return simple_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = portraitsCycle(X_train_sampled, y_train_sampled, X_cv, Y_cv, np.array(all_nplexes)[np.array(all_nplexes).sum(1) == 1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.437637037037\n"
     ]
    }
   ],
   "source": [
    "_ = portraitsCycle(X_train_sampled, y_train_sampled, X_cv, Y_cv, np.array(all_nplexes)[np.array(all_nplexes).sum(1) <= 2], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Тут по датасету создаем матрицу корреляций, разбиением на несколько фолдов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addSimplePredFeatures(data, labels, nplex_set = np.array(all_nplexes)[np.array(all_nplexes).sum(1) == 1],\n",
    "                                                                          n_folds = 5):\n",
    "    folds = []\n",
    "    fold_size = data.shape[0] / n_folds\n",
    "     \n",
    "    for i in range(n_folds):\n",
    "        fold = [i*fold_size, (i+1)*fold_size]\n",
    "        folds.append(fold)\n",
    "    folds[n_folds - 1][1] = data.shape[0]\n",
    "    \n",
    "    all_new_features = None\n",
    "    \n",
    "    for i, j in folds:\n",
    "        train_data = sparse.vstack((data[0:i],data[j:]))\n",
    "        #train_data = np.delete(data, np.s_[i : j], axis = 0)\n",
    "        train_labels = np.delete(labels, np.s_[i : j], axis = 0)\n",
    "        data_to_predict = data[i:j]\n",
    "        predict_labels = labels[i:j]\n",
    "        \n",
    "        #print train_data.shape\n",
    "        new_features = portraitsCycle(train_data, train_labels, data_to_predict, predict_labels, nplex_set)\n",
    "\n",
    "        if (all_new_features is None):\n",
    "            all_new_features = new_features\n",
    "        else:\n",
    "\n",
    "            all_new_features = np.vstack((all_new_features, new_features))\n",
    "        \n",
    "    \n",
    "    return all_new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше пробуем что из этого получится на кроссвалидации по выделенному изначально датасету( это не нужно сейчас, но тут какой-то порядок оценок для алгоритмов получен) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_features = addSimplePredFeatures(X_train_sampled, y_train_sampled, np.array(all_nplexes)[np.array(all_nplexes).sum(1) == 1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_features = np.nan_to_num(new_features)\n",
    "new_features_normed = (new_features / new_features.sum(1).reshape(-1,1))\n",
    "new_features_normed = np.nan_to_num(new_features_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_features_RFclassifier = RFClassifier(n_jobs = 6)\n",
    "new_features_RFclassifier.fit(new_features_normed, y_train_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_features_cv = predictSimple(X_cv, portraits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_features_cv = np.nan_to_num(new_features_cv)\n",
    "new_features_cv_normed = (new_features_cv / new_features_cv.sum(1).reshape(-1,1))\n",
    "new_features_cv_normed = np.nan_to_num(new_features_cv_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_proba = new_features_RFclassifier.predict_proba(new_features_cv_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nplexProbToClassProb(y, nplexes):\n",
    "    #!! only works for 1-plexes\n",
    "    ans = np.zeros((y.shape[0], 98))\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(len(nplexes)):\n",
    "            j_ind = np.where(nplexes[j] == 1)[0][0]\n",
    "            ans[i][j_ind] = y[i][j]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ansFromProba(y):\n",
    "    ans = np.zeros(y.shape)\n",
    "    ans[np.arange(y.shape[0]), y.argmax(1)] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_features_proba= nplexProbToClassProb(new_features_cv_normed, np.array(all_nplexes)[np.array(all_nplexes).sum(1) == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Вот кое-какие результаты. Сначала лес на матрице корреляций, потом просто максимум по корреляциям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.567133333333\n"
     ]
    }
   ],
   "source": [
    "print f1_score(Y_cv, ansFromProba(RF_proba), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519407407407\n"
     ]
    }
   ],
   "source": [
    "print f1_score(Y_cv, ansFromProba(new_features_proba), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = np.around(LinReg.predict(X_cv_PCA) + 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Пробуем подобрать коэффициенты на RF \\ corr \\ SGD \\ MNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно самая главная функция которая делает разбиение на три фолда для кроссвалидации. По каждым двум фолдам считаются корреляции (разбиением еще на 5 фолдов) потом учатся модели, и дается 4 предсказания (от каждой модели) для 3-его внешнего фолда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AllFolds(data, labels, nplexes, n_folds = 3) :\n",
    "    folds = []\n",
    "    fold_size = data.shape[0] / n_folds\n",
    "     \n",
    "    for i in range(n_folds):\n",
    "        fold = [i*fold_size, (i+1)*fold_size]\n",
    "        folds.append(fold)\n",
    "    \n",
    "    folds[n_folds - 1][1] = data.shape[0]\n",
    "    \n",
    "    folds_probas = []\n",
    "    \n",
    "    for i, j in folds:\n",
    "        probas = dict()\n",
    "        \n",
    "        train_data = sparse.vstack((data[0:i],data[j:]))\n",
    "        train_labels = np.delete(labels, np.s_[i : j], axis = 0)\n",
    "        cv_data = data[i:j]\n",
    "        cv_labels = labels[i:j]\n",
    "\n",
    "        new_features = addSimplePredFeatures(train_data, train_labels, nplexes, 5)\n",
    "        new_features = np.nan_to_num(new_features)\n",
    "        new_features_normed = (new_features / new_features.sum(1).reshape(-1,1))\n",
    "        new_features_normed = np.nan_to_num(new_features_normed)\n",
    "        \n",
    "        print 'teaching RF ......'\n",
    "        new_features_RFclassifier = RFClassifier(n_estimators = 100, n_jobs = 6)\n",
    "        new_features_RFclassifier.fit(new_features_normed, train_labels)\n",
    "        \n",
    "        print 'RF done...... \\n portraits.....'\n",
    "        portraits = createPortraits(nplexes, train_data, train_labels, False)\n",
    "        print 'done......'\n",
    "        cv_features = predictSimple(cv_data, portraits)\n",
    "        \n",
    "        cv_features = np.nan_to_num(cv_features)\n",
    "        cv_features_normed = (cv_features / cv_features.sum(1).reshape(-1,1))\n",
    "        cv_features_normed = np.nan_to_num(cv_features_normed)\n",
    "        \n",
    "        cv_features_proba = nplexProbToClassProb(cv_features_normed, nplexes)\n",
    "        \n",
    "        RF_proba = new_features_RFclassifier.predict_proba(cv_features_normed)\n",
    "        \n",
    "        SGD = SGDMulticlassClassifier()\n",
    "        \n",
    "        SGD.fit(train_data, train_labels)\n",
    "        SGD_proba = SGD.predict_proba(cv_data)\n",
    "        \n",
    "        MNB = MultinomialNB()\n",
    "        train_data_split, train_label_split = splitData(train_data, train_labels)\n",
    "        MNB.fit(train_data_split, train_label_split.argmax(1))\n",
    "        \n",
    "        MNB_labels = np.unique(sorted(train_labels.argmax(1)))\n",
    "        MNB_proba = updateMNBproba(MNB_labels, np.arange(98), MNB.predict_proba(cv_data))\n",
    "        \n",
    "        print 'All probas calculated....\\n'\n",
    "        probas['features_normed'] = cv_features_normed\n",
    "        probas['features_proba'] = cv_features_proba\n",
    "        probas['RF_proba'] = RF_proba\n",
    "        probas['SGD_proba'] = SGD_proba\n",
    "        probas['MNB_proba'] = MNB_proba\n",
    "        \n",
    "        folds_probas.append(probas)\n",
    "    \n",
    "    return folds_probas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39645\n",
      "0.482726190476\n",
      "0.47815\n",
      "0.463566666667\n",
      "0.4361\n",
      "teaching RF ......\n",
      "RF done...... \n",
      " portraits.....\n",
      "done......\n",
      "(5000, 98)\n",
      "All probas calculated....\n",
      "\n",
      "0.432833333333\n",
      "0.475576190476\n",
      "0.453816666667\n",
      "0.452316666667\n",
      "0.438983333333\n",
      "teaching RF ......\n",
      "RF done...... \n",
      " portraits.....\n",
      "done......\n",
      "(5000, 98)\n",
      "All probas calculated....\n",
      "\n",
      "0.428783333333\n",
      "0.487942857143\n",
      "0.465833333333\n",
      "0.45045952381\n",
      "0.443833333333\n",
      "teaching RF ......\n",
      "RF done...... \n",
      " portraits.....\n",
      "done......\n",
      "(5000, 98)\n",
      "All probas calculated....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_probas = AllFolds(X_train, y_train, np.array(all_nplexes)[np.array(all_nplexes).sum(1) == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняю данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "\n",
    "    to_save = np.zeros((5000, 1))\n",
    "\n",
    "    for key in f_probas[i]:\n",
    "        if key != 'features_normed':\n",
    "            to_save = np.hstack((to_save, f_probas[i][key]))\n",
    "    \n",
    "    pd.DataFrame(to_save).to_csv('fin_data_' + str(i)+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция генерит словарик весов в заданных диапазонах, чтобы подобрать оптимальное соотношение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genParams(step= 0.05, start = [0.4,0.3,0], end = [0.8,0.8,1]):\n",
    "    params_list = []\n",
    "    params = dict()\n",
    "    for i1 in np.arange(start[0],end[0] + 0.001,step):\n",
    "        for i2 in np.arange(start[1],min(1 - i1, end[1]) + 0.001,step):\n",
    "            for i3 in np.arange(start[2],min(1 - i1 - i2, end[2]) + 0.001,step):\n",
    "                params = dict()\n",
    "                params['RF_proba'] = i1\n",
    "                params['features_proba'] = i2\n",
    "                params['SGD_proba'] = i3\n",
    "                params['MNB_proba'] = 1 - i1 - i2 - i3\n",
    "                params_list.append(params)\n",
    "                \n",
    "    return params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_list = genParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Дальше вариации с выводом ответа (по трешхолду, по длине, по одному максимуму). В качестве решения на кагл принят вариант с предсказанием одного тега и предсказанием по длине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, mean: 0.515462539683\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "counter = 0\n",
    "for params in params_list[81:82]:\n",
    "    s = 0\n",
    "\n",
    "    for i in range(3):\n",
    "        ans = np.zeros((5000, 98))\n",
    "        for key in params:\n",
    "            ans += params[key] * f_probas[i][key]\n",
    "\n",
    "        s += f1_score(y_folds[i], ansFromProba(ans) , average='samples')\n",
    "        #print f1_score(y_folds[i], ansFromProba(ans) , average='samples')\n",
    "\n",
    "    print 'i: {1}, mean: {0}'.format(s/ 3, counter)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ansFromProba2(y):\n",
    "    y_copy = np.copy(y)\n",
    "    ans = np.zeros(y_copy.shape)\n",
    "    ans[np.arange(y.shape[0]), y_copy.argmax(1)] = 1\n",
    "    \n",
    "    y_copy[range(y.shape[0]), y.argmax(1)] = 0\n",
    "    ans[np.arange(y.shape[0]), y_copy.argmax(1)] = 1\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут пытался увеличить вероятности некотоых тегов после предсказания первого (матрица improvments). Считал ее наивно увеличивая вероятности тем тегам, которые встречабтся с данным чаще всего. Это немного лучше, чем длины, но я почему-то это не отправил.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addSecond(ans, proba, decisioner, improvment, t):\n",
    "    proba_cut = np.copy(proba)\n",
    "    proba_cut[np.arange(decisioner.shape[0]), decisioner.argmax(1)] = 0\n",
    "    \n",
    "    #print np.sort(proba_cut, axis = None)[-100:]\n",
    "    proba_cut += improvment[decisioner.argmax(1)] ** 1/4  *3\n",
    "    for i in range(ans.shape[0]):\n",
    "        if any(proba_cut[i] >= t):\n",
    "            #print 'any!'\n",
    "            ans[i][proba_cut[i] >= t] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.512201919192\n",
      "0.527317516519\n",
      "0.51471047619\n",
      " mean: 0.5180766373\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "t = 0.35\n",
    "\n",
    "for i in range(3):\n",
    "    ans = np.zeros((5000, 98))\n",
    "    for key in params:\n",
    "        ans += best_params[key] * f_probas[i][key]\n",
    "\n",
    "    ones_ans = ansFromProba(ans)\n",
    "    s += f1_score(y_folds[i], addSecond(ansFromProba(ans), ans, ans, improvment, t) , average='samples')\n",
    "    print f1_score(y_folds[i], addSecond(ansFromProba(ans), ans, ans, improvment, t) , average='samples')\n",
    "\n",
    "print ' mean: {0}'.format(s/ 3, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "add = 0.2\n",
    "c = 1\n",
    "lens_folds = []\n",
    "\n",
    "LinReg = Ridge(alpha=c, fit_intercept = True)\n",
    "reg_train = X_train_PCA[:10000]\n",
    "reg_label = y_train[:10000]\n",
    "LinReg.fit(reg_train, reg_label.sum(1))\n",
    "    \n",
    "lens_folds.append(np.round(LinReg.predict(X_train_PCA[10000:]) + add))\n",
    "\n",
    "LinReg = Ridge(alpha=c, fit_intercept = True)\n",
    "reg_train = np.vstack((X_train_PCA[:5000], X_train_PCA[10000:]))\n",
    "reg_label = np.vstack((y_train[:5000], y_train[10000:]))\n",
    "LinReg.fit(reg_train, reg_label.sum(1))\n",
    "\n",
    "lens_folds.append(np.round(LinReg.predict(X_train_PCA[5000:10000]) + add))\n",
    "\n",
    "LinReg = Ridge(alpha=c, fit_intercept = True)\n",
    "reg_train = X_train_PCA[5000:]\n",
    "reg_label = y_train[5000:]\n",
    "LinReg.fit(reg_train, reg_label.sum(1))\n",
    "    \n",
    "lens_folds.append(np.round(LinReg.predict(X_train_PCA[:5000]) + add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5476.0"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens_folds[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 98)\n",
      "(5000, 98)\n",
      "0.500986103896\n",
      "(5000, 98)\n",
      "(5000, 98)\n",
      "0.516225281385\n",
      "(5000, 98)\n",
      "(5000, 98)\n",
      "0.496389047619\n",
      " mean: 0.504533477633\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in range(3):\n",
    "    ans = np.zeros((5000, 98))\n",
    "    for key in params:\n",
    "        ans += best_params[key] * f_probas[i][key] \n",
    "    \n",
    "    ans[range(ans.shape[0]), ans.argmax(1)] = 100\n",
    "    ans += improvment[ans.argmax(1)] ** 1/2\n",
    "    s += f1_score(y_folds[i], produceOutByLen(ans, lens_folds[i].reshape(-1,1)) , average='samples')\n",
    "    print f1_score(y_folds[i], produceOutByLen(ans, lens_folds[i].reshape(-1,1)), average='samples')\n",
    "\n",
    "print ' mean: {0}'.format(s/ 3, counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "линейная мета модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4458\n",
      "0.501466666667\n",
      "0.515233333333\n",
      " mean: 0.4875\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in range(3):\n",
    "    \n",
    "    RF = RFClassifier(n_estimators=50, n_jobs=6)\n",
    "    \n",
    "    data = np.ones((5000, 1))\n",
    "    for key in params:\n",
    "        data = np.hstack((data,f_probas[i][key]))\n",
    "    \n",
    "    train = data[1000:]\n",
    "    test = data[:1000]\n",
    "    \n",
    "    RF.fit(train, y_folds[i][1000:])\n",
    "    \n",
    "    \n",
    "    s += f1_score(y_folds[i][:1000], ansFromProba(RF.predict_proba(test)) , average='samples')\n",
    "    print f1_score(y_folds[i][:1000], ansFromProba(RF.predict_proba(test)), average='samples')\n",
    "\n",
    "print ' mean: {0}'.format(s/ 3, counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "добавить, как фичу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.529076190476\n",
      "0.534083333333\n",
      "0.48935\n",
      " mean: 0.517503174603\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "t = 0.35\n",
    "th = 3000\n",
    "\n",
    "aaaaa = []\n",
    "rfff = []\n",
    "for i in range(3):\n",
    "    ans = np.zeros((5000, 98))\n",
    "    for key in best_params:\n",
    "        ans += best_params[key] * f_probas[i][key]\n",
    "\n",
    "    ones_ans = ansFromProba(ans)\n",
    "    \n",
    "    data = np.hstack((ans, ones_ans))\n",
    "    \n",
    "    train = data[:th]\n",
    "    test = data[th:] \n",
    "    \n",
    "    train_label = y_folds[i][:th]\n",
    "    test_label = y_folds[i][th:]\n",
    "    \n",
    "    train_label = train_label - ansFromProba(ans[:th])\n",
    "    train_label[train_label < 0] = 0\n",
    "    \n",
    "    RF_meta = RFClassifier(n_estimators = 50, n_jobs = 6)\n",
    "    RF_meta.fit(train, train_label)\n",
    "    \n",
    "    rf_pred = RF_meta.predict(test)\n",
    "    \n",
    "    aaaaa.append(ans)\n",
    "    rfff.append(rf_pred)\n",
    "    s += f1_score(test_label, np.round((ansFromProba(ans[th:]) + postprocessWithT(rf_pred, 0.5))/2 +0.1 ), average='samples')\n",
    "    print f1_score(test_label, np.round((ansFromProba(ans[th:]) + postprocessWithT(rf_pred, 0.5))/2 + 0.1 ), average='samples')\n",
    "\n",
    "print ' mean: {0}'.format(s/ 3, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MNB_proba': 9.7144514654701197e-17,\n",
       " 'RF_proba': 0.64999999999999991,\n",
       " 'SGD_proba': 0.050000000000000003,\n",
       " 'features_proba': 0.29999999999999999}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.515462539683"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add = 0.2\n",
    "c = 1\n",
    "\n",
    "LinReg = Ridge(alpha=c, fit_intercept = True)\n",
    "reg_train = X_train_PCA\n",
    "reg_label = y_train\n",
    "LinReg.fit(reg_train, reg_label.sum(1))\n",
    "    \n",
    "lens = (np.round(LinReg.predict(X_test) + add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrepareData(train_data, train_labels, test_data, nplexes) :\n",
    "    probas = dict()\n",
    "    new_features = addSimplePredFeatures(train_data, train_labels, nplexes, 5)\n",
    "    new_features = np.nan_to_num(new_features)\n",
    "    new_features_normed = (new_features / new_features.sum(1).reshape(-1,1))\n",
    "    new_features_normed = np.nan_to_num(new_features_normed)\n",
    "        \n",
    "    print 'teaching RF ......'\n",
    "    new_features_RFclassifier = RFClassifier(n_estimators = 1000, n_jobs = 6)\n",
    "    new_features_RFclassifier.fit(new_features_normed, train_labels)\n",
    "        \n",
    "    print 'RF done...... \\n portraits.....'\n",
    "    portraits = createPortraits(nplexes, train_data, train_labels, False)\n",
    "    print 'done......'\n",
    "    test_features = predictSimple(test_data, portraits)\n",
    "        \n",
    "    test_features = np.nan_to_num(test_features)\n",
    "    test_features_normed = (test_features / test_features.sum(1).reshape(-1,1))\n",
    "    test_features_normed = np.nan_to_num(test_features_normed)\n",
    "    \n",
    "    test_features_proba = nplexProbToClassProb(test_features_normed, nplexes)\n",
    "        \n",
    "    RF_proba = new_features_RFclassifier.predict_proba(test_features_normed)\n",
    "        \n",
    "    SGD = SGDMulticlassClassifier()\n",
    "        \n",
    "    SGD.fit(train_data, train_labels)\n",
    "    SGD_proba = SGD.predict_proba(test_data)\n",
    "        \n",
    "    MNB = MultinomialNB()\n",
    "    train_data_split, train_label_split = splitData(train_data, train_labels)\n",
    "    MNB.fit(train_data_split, train_label_split.argmax(1))\n",
    "        \n",
    "    MNB_labels = np.unique(sorted(train_labels.argmax(1)))\n",
    "    MNB_proba = updateMNBproba(MNB_labels, np.arange(98), MNB.predict_proba(test_data))\n",
    "        \n",
    "    print 'All probas calculated....\\n'\n",
    "    probas['features_normed'] = test_features_normed\n",
    "    probas['features_proba'] = test_features_proba\n",
    "    probas['RF_proba'] = RF_proba\n",
    "    probas['SGD_proba'] = SGD_proba\n",
    "    probas['MNB_proba'] = MNB_proba\n",
    "        \n",
    "\n",
    "    return probas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.466566666667\n",
      "0.471395238095\n",
      "0.479895238095\n",
      "0.478655555556\n",
      "0.448777777778\n",
      "teaching RF ......\n",
      "RF done...... \n",
      " portraits.....\n",
      "done......\n",
      "(15000, 98)\n",
      "All probas calculated....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kaggle_probas = PrepareData(X_train, y_train, X_test, np.array(all_nplexes)[np.array(all_nplexes).sum(1) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaggle_to_save = np.zeros((X_test.shape[0], 1))\n",
    "for key in kaggle_probas:\n",
    "    if key != 'features_normed':\n",
    "        kaggle_to_save = np.hstack((kaggle_to_save, kaggle_probas[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(kaggle_to_save).to_csv('kaggle_all_fin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = np.zeros((X_test.shape[0], 98))\n",
    "\n",
    "for key in best_params:\n",
    "    ans += best_params[key] * kaggle_probas[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15590.0"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 98)\n"
     ]
    }
   ],
   "source": [
    "write_labels_to_file(produceOutByLen(ans, lens.reshape(-1,1)), 'rf+corrs+SGD+MNB_by_lens.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот и все. Возможно можно было чуть получше выжать из этого сочетания моделей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
